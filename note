- scrapy的基本使用
    - 环境的安装：
        - pip install scrapy
    - 使用 scrapy startproject 项目名  来创建爬虫项目
    - cd 项目名
    - 在子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.XXX.com
    - 执行工程文件：
        - scrapy crawl spiderName
- scrapy数据解析

- scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse方法的返回值存储到本地文件中
        - 注意：持久化存储对应的文本文件的类型只可以为：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
        - 指令：scrapy crawl xxx -o filepath
        - 好处：简洁高效便捷
        - 缺点：局限性比较强（数据只可以存储指定后缀的文本文件中）

    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据存储到item类型的对象中
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道类的process_item类中要将其接受到的item对象中存放的数据进行持久化存储操作
            - 在配置文件中开启管道
        - 好处：
            - 通用性强。

    - 面试题：将爬取到的数据一份存储到本地一份存储数据库，如何实现呢？
        - 管道文件中一个管道类对应的是将数据存储到的一种平台
        - 爬虫文件提交的item只会提交给管道文件中第一个被执行的管道类接收
        - process_item中的return item表示将item传递给下一个即将被执行的管道类

- 基于Spider全站数据爬取
    - 就是将网站中某个板块下的全部页码对应的页数数据进行爬取
    - 需求：爬取彼岸图网中的照片名称
    - 实现方式：
        - 将所有的url添加到start_urls列表（不推荐）
        - 自行请求发送（推荐）
            - 手动请求发送：
                - yield scrapy Request(url,callback):callback专门用作于数据解析

- 五大核心组件
    引擎（Scrapy）
        用来处理整个系统的数据流处理，触发事物（核心框架）
    调度器（Scheduler）
        用来接收引擎发过来的请求，压入队列中，并在引擎再次请求时候返回，可以想象成一个url（专区网页）
    下载器（Downloader）
    爬虫（Spiders）
    项目管道（Pipline）


- 请求参数
    - 使用场景：解析的数据不再同一张页面当中。（深度爬取）
    - 需求：爬取boss聘的岗位名称，岗位描述

- 图片数据爬取之ImagesPipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
        - 字符串：只需要基于xpath解析且提交管道进行持久化存储
        - 图片：xpath解析出图片src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据

    - ImagesPipline:
        - 只需要将img的src的属性值进行解析，提交到管道，管道会对图片的src进行请求发送获取图片的二进制类型的数据，而且还会帮我们持久化存储
    - 需求：爬取站长素材里的高清图片
    - 使用流程：
        - 数据解析（图片的地址）
        - 将存储图片地址的item提交到指定的管道
        - 在管道文件中定制一个基于ImagesPipeline的一个管道类
            - ImagesPipeline类的 get_media_requests（就是可以根据图片地址进行数据请求的）
            - file_path（指定图片存储路径） 方法
            - item_completed 方法 return item 返回给下一个即将被执行的管道
        - 在配置文件中：
            - 指定图片存储的目录：IMAGE_STORE = path
            - 指定开启的管道：自己定制的管道类


- 中间件
    - 下载中间件
        - 位置：引擎和下载器之间
        - 批量拦截整个工程中的所有请求和响应
        - 拦截请求：
            - UA伪装：process_request
            - 代理IP：process_exception:return request

        - 拦截响应：
            - 篡改响应数据，响应对象
            - 需求：爬取网易新闻中的新闻数据（标题和对应的内容）
                - 1.通过网易的新闻解析出五大板块对应的详情页的url（没有动态加载）
                - 2.每个板块对应的新闻标题都是动态加载出来的（动态加载）
                - 3.通过解析每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容

